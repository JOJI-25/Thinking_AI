# üß† Thinking_AI: Biologically Inspired Cognitive Architecture

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![Status](https://img.shields.io/badge/status-experimental-orange)

**Thinking_AI** is a research framework implementing a "Cognitive Agent" based on principles from Computational Neuroscience. It bridges the gap between biological intelligence and artificial intelligence.

Unlike standard Deep Reinforcement Learning (DRL) models (like DQN or PPO) which are often static "Function Approximators," this system implements **dynamic, biologically plausible mechanisms**:
1.  **Spiking Neural Networks (SNNs):** Information is temporal (spike timing), not just spatial.
2.  **Neuromodulated Plasticity:** Learning happens online via local rules (Hebbian) modulated by global dopamine signals, avoiding biologically implausible Backpropagation-through-Time.
3.  **System 2 Deliberation:** The agent implements a "pause-and-think" loop, allowing it to simulate futures using a World Model before acting.

---

## üèó System Architecture & Diagrams

The architecture mimics the functional layout of the mammalian brain.

### 1. The BioAgent Data Flow
This diagram illustrates how sensory spikes compete for consciousness (Global Workspace) and how the brain switches between reflexive acting and predictive planning.

```mermaid
graph TD
    subgraph Environment
    Input["Sensory Input<br>(Grid/Text)"] -->|Poisson Spikes| Encoder
    end

    subgraph "BioAgent (The Brain)"
    Encoder -->|Spike Trains| SNN["Spiking Neural Network<br>(Sensory Cortex)"]
    
    %% Global Workspace Theory
    SNN -->|Feedforward| GW["Global Workspace<br>(Conscious Bottleneck)"]
    GW -->|"Broadcast (Feedback)"| SNN
    GW -->|"Broadcast (Feedback)"| WM["World Model<br>(Hippocampus/Frontal)"]
    
    %% Action Loops
    GW -->|High Confidence| Motor["Motor Cortex<br>(Action Execution)"]
    
    %% System 2 Loop
    GW -- Low Confidence --> Inhibit["üõë Inhibition System"]
    Inhibit --> WM
    WM -->|Simulated State| Evaluator["Value Network<br>(Amygdala)"]
    Evaluator -->|"Good Outcome?"| Motor
    end
    
    Motor -->|Action| Environment
    
    style GW fill:#f9f,stroke:#333,stroke-width:2px
    style Inhibit fill:#ff9999,stroke:#333,stroke-width:2px
2. The "System 2" Deliberation LogicThe core innovation is the algorithm's ability to "imagine" the future when it is uncertain.Code snippetflowchart TD
    Start("State S_t") --> FastNet["System 1: Fast Policy"]
    FastNet -->|"Propose Action A"| Check{"Uncertainty > Threshold?"}
    
    %% Path 1: Reflexive Action
    Check -- "No (Confident)" --> Execute["Execute Action A"]
    
    %% Path 2: Deliberative Action
    Check -- "Yes (Confused)" --> Pause["üõë Pause Motor Output"]
    Pause --> Sim["World Model Simulation"]
    
    subgraph "Inner Monologue / Simulation"
    Sim -->|"Predict S_t+1"| Value{"Is State Safe?"}
    Value -- Yes --> Confirm["Confirm Action A"]
    Value -- No --> Rollout["Tree Search / Rollout"]
    Rollout -->|"Select Best A'"| Confirm
    end
    
    Confirm --> Execute
    Execute --> Next("State S_t+1")
‚öôÔ∏è Detailed Algorithmic MechanicsThe system operates in continuous time, discretized into steps ($dt$).1. Spiking Neuron Model (LIF)We use the Leaky Integrate-and-Fire (LIF) model to capture temporal dynamics.The membrane potential $V(t)$ of neuron $i$ evolves according to:$$ \tau_m \frac{dV_i}{dt} = -(V_i(t) - V_{rest}) + R \cdot \sum_{j} w_{ij} S_j(t) $$$\tau_m$: Membrane time constant (how fast voltage decays).$S_j(t)$: Incoming spikes from presynaptic neurons (0 or 1).Spike Condition: If $V_i(t) > V_{threshold}$, the neuron fires a spike ($S_i(t)=1$) and resets to $V_{reset}$.2. The Three-Factor Plasticity RuleStandard AI uses Backpropagation, which requires "freezing" the world to calculate gradients. Biology uses Eligibility Traces.Weight updates ($\Delta w$) depend on three factors:Presynaptic Spike: Neuron $j$ fired.Postsynaptic Spike: Neuron $i$ fired shortly after $j$ (Causal link).Dopamine (Reward): A global signal $D$ indicating success/failure.The Equation:$$ \frac{de_{ij}}{dt} = -\frac{e_{ij}}{\tau_e} + S_j(t) \cdot S_i(t) $$$$ \Delta w_{ij} = \eta \cdot e_{ij}(t) \cdot D(t) $$$e_{ij}$: The eligibility trace (a fading memory of "who fired together").$D(t)$: The reward or TD-error signal.$\eta$: Learning rate.Result: The network learns only when an action leads to a reward, reinforcing the specific synapses that caused that action.3. Global Workspace (Attention)The agent features a "Workspace" module.Competition: Multiple sensory modules (e.g., vision, memory) send spikes to the workspace.Bottleneck: Only the $k$ most active neurons in the workspace are allowed to fire ("Winner-Take-All").Broadcasting: The winners' spikes are sent back to all modules, effectively synchronizing the brain on a single "thought."4. System 2 ReasoningImplemented in agent.py, this loop triggers when the entropy (uncertainty) of the spike distribution is high.Step A (Inhibition): If uncertainty is high, the motor cortex is inhibited.Step B (Prediction): The current state vector is passed to the Transformer World Model (modules.py).Step C (Rollout): The model predicts $\hat{S}_{t+1}$ given action $a$.Step D (Evaluation): A value network estimates $V(\hat{S}_{t+1})$. If $V$ is negative (e.g., hitting a wall), the agent picks a different action internally before moving externally.üìÇ File StructureFileComponentDescriptionagent.pyBioAgentThe central class. Integrates SNN, Workspace, and the logic for the System 2 loop.neuron.pyPhysiologyContains the LIFNeuron class and differential equation solvers.plasticity.pyLearningImplements the trace-based Hebbian learning rules.workspace.pyConsciousnessImplements the competitive "Winner-Take-All" dynamics.modules.pyWorld ModelContains the Transformer used for future prediction.think.pyTrainingThe main loop for the Navigation/GridWorld task.train_generative.pyLanguageAdaptation of the agent for text generation tasks.config.pyHyperparametersGlobal settings (Time steps, Thresholds, Learning Rates).üöÄ Installation & Usage1. Setup EnvironmentBashgit clone [https://github.com/JOJI-25/Thinking_AI.git](https://github.com/JOJI-25/Thinking_AI.git)
cd Thinking_AI
pip install torch numpy matplotlib gymnasium
2. Run Navigation TrainingTrain the agent to solve a maze using only biological rules.Bashpython think.py
3. Visualize the "Thinking" ProcessRun the test script. The console output will visualize the agent's internal state. Look for the [Thinking...] log, which indicates the agent has paused to simulate the future.Bashpython test_agent.py
üîÆ Roadmap[ ] Hippocampal Replay: Implement "dreaming" (offline replay of high-reward traces) to improve sample efficiency.[ ] Visual Cortex: Replace GridWorld input with Spiking CNNs for pixel-based environments.[ ] Metacognition: Allow the agent to learn a cost function for when to think (balancing speed vs. accuracy).üìú LicenseDistributed under the MIT License.
